{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(os.getcwd(), \"datasets\", \"animals10\", \"raw-img\")\n",
    "class_list = os.listdir(path)\n",
    "class_list.remove('.DS_Store')\n",
    "data_paths = [os.listdir(os.path.join(path, c)) for c in class_list]\n",
    "sample_n = [len(c) for c in data_paths]\n",
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Dataset:\n",
    "    def __init__(self, BS, shuffle=True):\n",
    "        self.BS = BS\n",
    "        self.XIndex = list()\n",
    "        self.y = list()\n",
    "        for i, yi in enumerate(sample_n):\n",
    "            for xi in range(yi):\n",
    "                self.y.append(i)\n",
    "                self.XIndex.append(xi)\n",
    "        self.XIndex = np.array(self.XIndex)\n",
    "        self.y = np.array(self.y)\n",
    "        self.l = len(self.y)\n",
    "        if shuffle:\n",
    "            randl = list(range(self.l))\n",
    "            np.random.shuffle(randl)\n",
    "            self.XIndex = self.XIndex[randl]\n",
    "            self.y = self.y[randl]\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.l/self.BS))\n",
    "    def __getitem__(self, key):\n",
    "        start = key * self.BS\n",
    "        assert start < self.l\n",
    "        stop = start + self.BS\n",
    "        if stop > self.l and start < self.l:\n",
    "            stop = self.l\n",
    "        return self.XIndex[start:stop], self.y[start:stop]\n",
    "    def __iter__(self):\n",
    "        for b in range(len(self)):\n",
    "            start = b*self.BS\n",
    "            X, y = self.XIndex[start:start+self.BS], self.y[start:start+self.BS]\n",
    "            X_path = list()\n",
    "            for xs, ys in zip(X, y):\n",
    "                X_path.append(os.path.join(path,class_list[ys],data_paths[ys][xs]))\n",
    "            X_path = np.array(X_path)\n",
    "            X_Images = self.toMem(X_path)\n",
    "            yield X_Images, y\n",
    "    def toMem(self, X_path):\n",
    "        from PIL import Image\n",
    "        X_images = list()\n",
    "        for img_path in X_path:\n",
    "            im = Image.open(img_path)\n",
    "            X_images.append(im)\n",
    "        return X_images\n",
    "        \n",
    "'''\n",
    "getdata = Dataset(BS=150)\n",
    "for x_b, y_b in getdata:\n",
    "    print(type(x_b[0]))\n",
    "    # print(x_b, y_b, x_b.shape, y_b.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.manual_seed(17)\n",
    "from torchvision import datasets, transforms\n",
    "BS = 8\n",
    "IM_HEIGHT = 425\n",
    "IM_WIDTH = 600\n",
    "transform = transforms.Compose([transforms.Resize([IM_HEIGHT, IM_WIDTH]), transforms.ToTensor()])\n",
    "path = os.path.join(os.getcwd(), \"datasets\", \"animals10\", \"raw-img\")\n",
    "dataset = datasets.ImageFolder(path, transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.7), int(len(dataset)*0.3)+1])\n",
    "train_set_loader = torch.utils.data.DataLoader(train_set, batch_size=BS, shuffle=True)\n",
    "val_set_loader = torch.utils.data.DataLoader(val_set, batch_size=BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "i, l = next(iter(train_set_loader))\n",
    "print(i.shape, i[0].shape)\n",
    "sample = i[0][2]\n",
    "print(sample)\n",
    "plt.imshow(i[0][2].reshape(IM_HEIGHT, IM_WIDTH))\n",
    "plt.show()\n",
    "print(i[0][0])\n",
    "print(torch.min(i[0][0]))\n",
    "print(torch.max(i[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class someNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(IM_HEIGHT*IM_WIDTH*3, IM_HEIGHT*IM_WIDTH)\n",
    "        self.l2 = nn.Linear(IM_HEIGHT*IM_WIDTH, 10)\n",
    "        nn.init.normal_(self.l1.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.l2.weight, mean=0, std=0.1)\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "model = someNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAN = False\n",
    "N_EPOCH = 10\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WAN:\n",
    "  import wandb\n",
    "  wandb.init(project=\"animals10\", entity=\"0xasim\")\n",
    "  wandb.config = {\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": N_EPOCH,\n",
    "    \"batch_size\": BS\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "review = lambda x_b: x_b.view(len(x_b), -1)\n",
    "def getValLoss():\n",
    "    with torch.no_grad():\n",
    "        val_loss = list()\n",
    "        for x_v_b, y_v_b in tqdm(val_set_loader):\n",
    "            x_v_b = torch.tensor([torch.flatten(e_x).numpy() for e_x in x_v_b])\n",
    "            pred = model.forward(x_v_b)\n",
    "            loss = loss_fn(pred, y_v_b)\n",
    "            val_loss.append(loss.sum()/y_v_b.shape[0])\n",
    "            print(val_loss)\n",
    "        return val_loss.sum().item()\n",
    "\n",
    "def train():\n",
    "    for i in (range(N_EPOCH)):\n",
    "        batch_loss = list()\n",
    "        for x_b, y_b in tqdm(train_set_loader):\n",
    "            x_b = review(x_b)\n",
    "            pred = model.forward(x_b)\n",
    "            loss = loss_fn(pred, y_b)\n",
    "            # clear gradients for this training step\n",
    "            optimizer.zero_grad()\n",
    "            # backprop, compute gradients\n",
    "            loss.backward()\n",
    "            # apply gradients\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.sum()/y_b.shape[0])\n",
    "            if WAN:\n",
    "                wandb.log({\"train_loss\": loss.sum().item()})\n",
    "        if WAN:\n",
    "            wandb.log({\"val_loss\": getValLoss()})\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
